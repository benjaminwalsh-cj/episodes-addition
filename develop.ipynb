{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.basicConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline Test Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_scores(path: str, label: str) -> pd.DataFrame:\n",
    "    '''Load a precision_recall_fscore_file.xlsx from the path\n",
    "    Args:\n",
    "        path (`str`): path to evaluation scores file\n",
    "        label (`str`): label to distinguish the pipeline run\n",
    "    Returns:\n",
    "        Evaluation results dataframe formatted for further evaluation steps\n",
    "    Raises:\n",
    "        FileNotFoundError\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(path)\n",
    "    except FileNotFoundError:\n",
    "        logger.error('Path to evaluation file is incorrect.')\n",
    "        raise\n",
    "\n",
    "    df['label'] = label\n",
    "\n",
    "    return_df = df.melt(id_vars=['label', 'Category'], value_vars=['Precision', 'Recall', 'fscore'], value_name='values', var_name='measure')\n",
    "\n",
    "    return_df['values'] = return_df['values'].astype(float)\n",
    "\n",
    "    return return_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './all_egm/results/train/precision_recall_fscore/precision_recall_fscore_file.xlsx'\n",
    "\n",
    "post_df = load_evaluation_scores(path=path, label='all_egm')\n",
    "\n",
    "path = './no_egm/results/train/precision_recall_fscore/precision_recall_fscore_file.xlsx'\n",
    "\n",
    "pre_df = load_evaluation_scores(path=path, label='no_egm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_evaluation_scores(\n",
    "    pre_scores: pd.DataFrame,\n",
    "    post_scores: pd.DataFrame,\n",
    "    pre_label: str,\n",
    "    post_label: str) -> pd.DataFrame:\n",
    "    '''Create an evaluation dataframe to easily compare the evaluation results from\n",
    "    different pipeline runs\n",
    "    Args:\n",
    "        pre_scores (`pd.DataFrame`): testing evaluation results from base run\n",
    "        post_scores (`pd.DataFrame`): testing evaluation results from new run\n",
    "        pre_label (`str`): value of the `label` column in the pre dataframe\n",
    "        post_label (`str`): value of the `label` column in the post dataframe\n",
    "    Return:\n",
    "        Pandas dataframe for easier cross-runs comparisons of eval results\n",
    "    '''\n",
    "\n",
    "    if not isinstance(pre_scores, pd.DataFrame):\n",
    "        logger.error('Argument `pre_scores` is not a pandas dataframe.')\n",
    "        raise TypeError\n",
    "\n",
    "    if not isinstance(post_scores, pd.DataFrame):\n",
    "        logger.error('Argument `post_scores` is not a pandas dataframe.')\n",
    "        raise TypeError\n",
    "\n",
    "    # Merge on category and measure\n",
    "    merged_df = pre_scores.merge(post_scores, on=['Category', 'measure'])\n",
    "\n",
    "    # Drop label columns\n",
    "    merged_df = merged_df[['Category', 'measure', 'values_x', 'values_y']]\n",
    "\n",
    "    # Add delta\n",
    "    merged_df['delta'] = merged_df['values_y'] - merged_df['values_x']\n",
    "\n",
    "    # Flag if improved\n",
    "    merged_df['improve_flag'] = np.where(merged_df['delta'] > 0, 1, 0)\n",
    "\n",
    "    # Relabel columns\n",
    "    merged_df = merged_df.rename(\n",
    "        columns={\n",
    "            'values_x': f'{pre_label}_value',\n",
    "            'values_y': f'{post_label}_value'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return merged_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>measure</th>\n",
       "      <th>no_egm_value</th>\n",
       "      <th>all_egm_value</th>\n",
       "      <th>delta</th>\n",
       "      <th>improve_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.998331</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cardiology</td>\n",
       "      <td>Precision</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Other</td>\n",
       "      <td>Recall</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cardiology</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.996283</td>\n",
       "      <td>0.998141</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other</td>\n",
       "      <td>fscore</td>\n",
       "      <td>0.998331</td>\n",
       "      <td>0.999165</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cardiology</td>\n",
       "      <td>fscore</td>\n",
       "      <td>0.998138</td>\n",
       "      <td>0.999070</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category    measure  no_egm_value  all_egm_value     delta  improve_flag\n",
       "0       Other  Precision      0.996667       0.998331  0.001664             1\n",
       "1  Cardiology  Precision      1.000000       1.000000  0.000000             0\n",
       "2       Other     Recall      1.000000       1.000000  0.000000             0\n",
       "3  Cardiology     Recall      0.996283       0.998141  0.001859             1\n",
       "4       Other     fscore      0.998331       0.999165  0.000834             1\n",
       "5  Cardiology     fscore      0.998138       0.999070  0.000932             1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_evaluation_scores(pre_scores=pre_df, post_scores=post_df, pre_label='no_egm', post_label='all_egm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73edeb63486cc3f58abf326bbf1fe353a792c5ed6d86f6fcdfde081eecf3f816"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
